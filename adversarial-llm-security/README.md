![Dashboard](docs/dashboard_1.png)

# ğŸ” Adversarial LLM Security Simulator

A simulation framework for testing adversarial prompt injection attacks against Large Language Models (LLMs), with risk scoring, incident detection, and a Streamlit dashboard for monitoring results.

## ğŸš€ Project Overview

This project simulates an adversarial environment where:

- ğŸ¤– An **Attacker LLM** generates prompt injection attempts
- ğŸ›¡ A **Defender LLM** classifies inputs as benign or malicious
- ğŸ“Š A **Risk Engine** computes a dynamic risk score
- ğŸš¨ Incidents are flagged based on configurable thresholds
- ğŸ“§ An **incident_service** sends and email alerting the user
- ğŸ“ˆ A **Streamlit Dashboard** visualizes metrics and attack success rate

The goal is to study and measure LLM robustness against prompt injection attacks.

---

## ğŸ§  Architecture
adversarial-llm-security/
â”‚
â”œâ”€â”€ main.py # Simulation entrypoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ dashboard/
â”‚ â””â”€â”€ dashboard.py # Streamlit monitoring UI
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ _init_.py
| |â”€â”€ config.py
â”‚ â”œâ”€â”€ attacker.py 
â”‚ â”œâ”€â”€ controller.py
â”‚ â”œâ”€â”€ incident_service.py
â”‚ â”œâ”€â”€ defender.py
â”‚ â”œâ”€â”€ risk_engine.py
â”‚ â”œâ”€â”€ metrics.py
â”‚ â”œâ”€â”€ database.py
â”‚
â””â”€â”€ tests/
â”‚ â”œâ”€â”€ test_email.py


---

## âš™ï¸ Features

- Adversarial prompt generation
- Defender classification with confidence score
- Risk score computation
- Incident detection threshold
- Persistent logging (SQLite via SQLAlchemy)
- Real-time Streamlit dashboard
- Metrics computation:
  - Attack success rate
  - Average risk score
  - Incident count
  - Confidence distribution

---

## ğŸ“¦ Installation

Clone the repository:

```bash
git clone https://github.com/your-username/adversarial-llm-security.git
cd adversarial-llm-security

Create virtual environment:
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows

Install dependencies:
pip install -r requirements.txt

## â–¶ï¸ Run Simulation
python main.py

## ğŸ“Š Run Dashboard
streamlit run dashboard/dashboard.py
```
![Debug](docs/debug.png)

## ğŸ¯ Learning Goals

- LLM adversarial robustness

- AI security evaluation

- Risk modeling systems

- Clean project architecture

- Monitoring dashboards

- Alerting with emails

![Email](docs/attack_email.png)

Project schema:
![Project_Schema](docs/schemaa.png)

## âš ï¸ Disclaimer

This project is for educational and research purposes only.
It simulates adversarial behaviors to improve defensive AI systems.
